{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Tutorial\n",
    "\n",
    "There are many NLP methods that require tokenized data as input, such as machine translation and word alignment. In this notebook, we will show how to use the different tokenizers and detokenizers that are available in Machine. Tokenizers implement either the `ITokenizer` interface or the `IRangeTokenizer` interface. `ITokenizer` classes are used to segment a sequence into tokens. `IRangeTokenizer` classes return ranges that mark where each each token occurs in the sequence. Detokenizers implement the `IDetokenizer` interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>SIL.Scripture, 12.0.1</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget:SIL.Scripture,12.0.1\"\n",
    "#r \"../src/SIL.Machine/bin/Debug/netstandard2.0/SIL.Machine.dll\"\n",
    "#r \"../src/SIL.Machine.Tokenization.SentencePiece/bin/Debug/netstandard2.0/SIL.Machine.Tokenization.SentencePiece.dll\"\n",
    "\n",
    "void WriteLine(string text = \"\")\n",
    "{\n",
    "    Console.Write(text + \"\\n\");\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "Let's start with a simple, whitespace tokenizer. This tokenizer is used to split a string at whitespace. This tokenizer is useful for text that has already been tokenized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This | is | a | test | .\n"
     ]
    }
   ],
   "source": [
    "using SIL.Machine.Tokenization;\n",
    "\n",
    "var tokenizer = new WhitespaceTokenizer();\n",
    "var tokens = tokenizer.Tokenize(\"This is a test .\");\n",
    "WriteLine(string.Join(\" | \", tokens));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine contains general tokenizers that can be used to tokenize text from languages with a Latin-based script. A word tokenizer and a sentence tokenizer are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer | scelerisque | efficitur | dui | , | eu | tincidunt | erat | posuere | in | .\n",
      "Curabitur | vel | finibus | mi | .\n"
     ]
    }
   ],
   "source": [
    "var sentenceTokenizer = new LatinSentenceTokenizer();\n",
    "var sentences = sentenceTokenizer.Tokenize(\n",
    "    \"Integer scelerisque efficitur dui, eu tincidunt erat posuere in. Curabitur vel finibus mi.\");\n",
    "var wordTokenizer = new LatinWordTokenizer();\n",
    "WriteLine(string.Join(\"\\n\", sentences.Select(s => string.Join(\" | \", wordTokenizer.Tokenize(s)))));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tokenizers implement the `IRangeTokenizer` interface. These tokenizers have an additional method, `TokenizeAsRanges`, that returns ranges that mark the position of all tokens in the original string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"][This] [is] [a] [test][,] [also][.][\"]\n"
     ]
    }
   ],
   "source": [
    "var wordTokenizer = new LatinWordTokenizer();\n",
    "var sentence = \"\\\"This is a test, also.\\\"\";\n",
    "var ranges = wordTokenizer.TokenizeAsRanges(sentence);\n",
    "var output = \"\";\n",
    "var prev_end = 0;\n",
    "foreach (var range in ranges)\n",
    "{\n",
    "    output += sentence.Substring(prev_end, range.Start - prev_end);\n",
    "    output += $\"[{sentence.Substring(range.Start, range.Length)}]\";\n",
    "    prev_end = range.End;\n",
    "}\n",
    "WriteLine(output + sentence.Substring(prev_end));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some languages that do not delimit words with spaces, but instead delimit sentences with spaces. In these cases, it is common practice to use zero-width spaces to explicitly mark word boundaries. This is often done for Bible translations. Machine contains a word tokenizer that is designed to properly deal with text use zero-width space to delimit words and spaces to delimit sentences. Notice that the space is preserved, since it is being used as punctuation to delimit sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem | Ipsum | Dolor | Sit | Amet | Consectetur |   | Adipiscing | Elit | Sed\n"
     ]
    }
   ],
   "source": [
    "var wordTokenizer = new ZwspWordTokenizer();\n",
    "var tokens = wordTokenizer.Tokenize(\"Lorem​Ipsum​Dolor​Sit​Amet​Consectetur Adipiscing​Elit​Sed\");\n",
    "WriteLine(string.Join(\" | \", tokens));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenization has become popular for use with deep learning models. Machine provides a [SentencePiece](https://github.com/google/sentencepiece) tokenizer that can perform both BPE and unigram subword tokenization. Another advantage of subword tokenization is that it is language-independent and allows one to specify the size of the vocabulary. This helps to deal with out-of-vocabulary issues. First, let's train a SentencePiece model. SentencePiece classes are implemented in the [SIL.Machine.Tokenization.SentencePiece](https://www.nuget.org/packages/SIL.Machine.Tokenization.SentencePiece/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using System.IO;\n",
    "using SIL.Machine.Tokenization.SentencePiece;\n",
    "\n",
    "Directory.CreateDirectory(\"out\");\n",
    "var trainer = new SentencePieceTrainer\n",
    "{\n",
    "    VocabSize = 400,\n",
    "    ModelType = SentencePieceModelType.Unigram\n",
    "};\n",
    "trainer.Train(\"data/en.txt\", \"out/en-sp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a SentencePiece model, we can split the text into subwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Th | is | ▁ | is | ▁a | ▁ | t | es | t | .\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    using var tokenizer = new SentencePieceTokenizer(\"out/en-sp.model\");\n",
    "    var tokens = tokenizer.Tokenize(\"This is a test.\");\n",
    "    WriteLine(string.Join(\" | \", tokens));\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detokenizing text\n",
    "\n",
    "For many NLP pipelines, tokens will need to be merged back into detokenized text. This is very common for machine translation. Many of the tokenizers in Machine also have a corresponding detokenizer that can be used to convert tokens back into a correct sequence. Once again, let's start with a simple, whitespace detokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test .\n"
     ]
    }
   ],
   "source": [
    "var detokenizer = new WhitespaceDetokenizer();\n",
    "var sentence = detokenizer.Detokenize(new[] { \"This\", \"is\", \"a\", \"test\", \".\" });\n",
    "WriteLine(sentence);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine has a general detokenizer that works well with languages with a Latin-based script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is a test, also.\"\n"
     ]
    }
   ],
   "source": [
    "var wordDetokenizer = new LatinWordDetokenizer();\n",
    "var sentence = wordDetokenizer.Detokenize(new[] { \"\\\"\", \"This\", \"is\", \"a\", \"test\", \",\", \"also\", \".\", \"\\\"\" });\n",
    "WriteLine(sentence);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine has a detokenizer that properly deals with text that uses zero-width space to delimit words and spaces to delimit sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem​Ipsum​Dolor​Sit​Amet​Consectetur Adipiscing​Elit​Sed\n"
     ]
    }
   ],
   "source": [
    "var wordDetokenizer = new ZwspWordDetokenizer();\n",
    "var sentence = wordDetokenizer.Detokenize(\n",
    "    new[] { \"Lorem\", \"Ipsum\", \"Dolor\", \"Sit\", \"Amet\", \"Consectetur\", \" \", \"Adipiscing\", \"Elit\", \"Sed\" });\n",
    "WriteLine(sentence);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine contains a detokenizer for SentencePiece encoded text. SentencePiece encodes spaces in the tokens, so that it can be detokenized without any ambiguities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "var detokenizer = new SentencePieceDetokenizer();\n",
    "var sentence = detokenizer.Detokenize(new[] { \"▁Th\", \"is\", \"▁\", \"is\", \"▁a\", \"▁\", \"t\", \"es\", \"t\", \".\" });\n",
    "WriteLine(sentence);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "C#"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
